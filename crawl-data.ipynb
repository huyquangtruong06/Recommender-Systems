{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785ff113-1c37-4e8c-872a-b689c81c7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in e:\\anaconda\\setup\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\setup\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ce4245-50ac-46e5-a1a8-bb5e0a11250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6245cf-bb4d-4c47-ada2-cd0d33eea021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "CONFIG = {\n",
    "    \"container\": \".listproduct\",  \n",
    "    \"item_tag\": \"a\",             \n",
    "    \"see_more_btn\": \".see-more-btn\",\n",
    "    \"next_btn_id\": \"_pgNextPage\",\n",
    "    \"disabled_class\": \"disabled\"\n",
    "}\n",
    "\n",
    "# read data from file csv and access link\n",
    "INPUT_FILE = 'list_link_product.csv'  # input\n",
    "OUTPUT_FILE = 'reviews_rating.csv' # output\n",
    "MAX_WORKERS = 4    \n",
    "MAX_PAGES = 5\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\") # Chạy ngầm\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # turn of img.\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # fake user-agent\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "    \n",
    "    \n",
    "def random_sleep(min_s=1.0, max_s=2.0):\n",
    "    time.sleep(random.uniform(min_s, max_s))\n",
    "\n",
    "def extract_links(driver, list_link_product):\n",
    "    try : \n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, CONFIG[\"container\"])))\n",
    "        container = driver.find_element(By.CSS_SELECTOR, CONFIG[\"container\"])\n",
    "        elements = container.find_elements(By.TAG_NAME, CONFIG[\"item_tag\"])\n",
    "        for el in elements : \n",
    "            try :\n",
    "                link = el.get_attribute(\"href\")\n",
    "                if link and \"http\" in link :\n",
    "                    list_link_product.add(link)\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        return \n",
    "        \n",
    "def process_see_more(driver, url, list_link) :\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10) \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, CONFIG[\"see_more_btn\"])))\n",
    "            # Scroll\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", btn)\n",
    "            time.sleep(0.5) \n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            random_sleep(1.5, 2.5) # load\n",
    "            \n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            break\n",
    "        except Exception as e:\n",
    "                break\n",
    "    # crawl data\n",
    "    extract_links(driver, list_link)\n",
    "    \n",
    "def process_pagination(driver, url, list_link) :\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10) \n",
    "    \n",
    "    while True:\n",
    "        # crawl data\n",
    "        extract_links(driver, list_link)\n",
    "        try :\n",
    "            # move page.\n",
    "            next_li = driver.find_element(By.ID, CONFIG[\"next_btn_id\"])\n",
    "            if CONFIG[\"disabled_class\"] in next_li.get_attribute(\"class\"):\n",
    "                break\n",
    "                \n",
    "            # Click next\n",
    "            next_a = next_li.find_element(By.TAG_NAME, \"a\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_a)\n",
    "            random_sleep(2, 3)\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def scrape_reviews(product_url):\n",
    "    driver = init_driver()\n",
    "    \n",
    "    if \"/danh-gia\" not in product_url:\n",
    "        if \"?\" in product_url:\n",
    "             review_url = product_url.split(\"?\")[0] + \"/danh-gia\"\n",
    "        else:\n",
    "             review_url = product_url + \"/danh-gia\"\n",
    "    else:\n",
    "        review_url = product_url\n",
    "\n",
    "    \n",
    "    all_reviews = []\n",
    "    try : \n",
    "        driver.get(review_url)\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, \"comment-list\")))\n",
    "        except TimeoutException:\n",
    "            driver.quit()\n",
    "            return []\n",
    "        \n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "  \n",
    "            # get current HTML\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            # get list <li> comment (class=\"par\")\n",
    "            review_items = soup.find_all('li', class_='par')\n",
    "            \n",
    "            if not review_items:\n",
    "                break\n",
    "            for item in review_items:\n",
    "                try:\n",
    "                    # user\n",
    "                    name_tag = item.find(class_='cmt-top-name')\n",
    "                    user_name = name_tag.get_text(strip=True) if name_tag else \"Anonymous\"\n",
    "                    \n",
    "                    # comment\n",
    "                    content_tag = item.find(class_='cmt-txt')\n",
    "                    content = content_tag.get_text(strip=True) if content_tag else \"\"\n",
    "                    \n",
    "                    # rating\n",
    "                    stars_container = item.find(class_='cmt-top-star')\n",
    "                    if stars_container:\n",
    "                        stars = len(stars_container.find_all('i', class_='iconcmt-starbuy'))\n",
    "                    else:\n",
    "                        stars = 0 \n",
    "                    \n",
    "                    product_id = product_url\n",
    "                    \n",
    "                    if content: \n",
    "                        all_reviews.append({\n",
    "                            \"user_name\": user_name,\n",
    "                            \"product_id\": product_url,\n",
    "                            \"rating\": stars,\n",
    "                            \"comment\": content\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue  \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    finally:\n",
    "        driver.quit() \n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55398eb-1dcb-4563-9a87-4228c3d9171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "information_url = {\n",
    "    \"https://www.thegioididong.com/dtdd\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/laptop\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/may-tinh-bang\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/pc-may-in\" : \"SeeMore\",\n",
    "    \n",
    "    \"https://www.thegioididong.com/dong-ho-thong-minh-da-tien-ich\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/dong-ho-thong-minh-the-thao-chuyen-nghiep\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/dong-ho-thong-minh-tre-em\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/day-dong-ho\" : \"SeeMore\",\n",
    "    \n",
    "    \"https://www.thegioididong.com/camera-giam-sat\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/tai-nghe\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/sac-dtdd\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/phu-kien/apple\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/loa-laptop\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/sac-cap\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/chuong-trinh-phu-kien-laptop\" : \"SeeMore\",\n",
    "\n",
    "    \"https://www.thegioididong.com/bang-ve-dien-tu\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/mieng-lot-chuot\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/den-dien-den-sac\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/gia-treo-man-hinh\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/phan-mem\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/mieng-phu-ban-phim\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/tui-dung-phu-kien\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/tui-chong-soc\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/thiet-bi-mang\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/ban-phim\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/chuot-may-tinh\" : \"SeeMore\",\n",
    "    \"https://www.thegioididong.com/hub-chuyen-doi\" : \"SeeMore\",\n",
    "\n",
    "    # Pagination\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay#c=7264&o=8&pi=0\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-nam\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-nu\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-casio\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-citizen\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-orient\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/khuyen-mai-dong-ho-chi-ban-online\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-lacoste?itm_source=trang-nganh-hang&itm_medium=filter\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-tommy\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-festina\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-coach\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-bulova\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-ferrari\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-candino\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-movado\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-certina\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-ernest-borel\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-citizen\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-edifice-casio\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-orient-star\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-casio-protrek\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-g-shock\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-MVW\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-elio\" : \"Pagination\",\n",
    "    \"https://www.thegioididong.com/dong-ho-deo-tay-tre-em\" : \"Pagination\",\n",
    "    # Pagination\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bde42-8c76-46a4-831b-b2a6a4c5b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    list_link_product = set()\n",
    "    # init driver\n",
    "    driver = init_driver()\n",
    "    try :\n",
    "        for url, flag in information_url.items():\n",
    "            if flag == \"SeeMore\" : \n",
    "                process_see_more(driver, url, list_link_product)\n",
    "            else :\n",
    "                process_pagination(driver, url, list_link_product)      \n",
    "    finally :\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "    print(len(list_link_product))\n",
    "    # save\n",
    "    with open(\"list_link_product.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"product_link\"])   # header\n",
    "        for link in list_link_product:\n",
    "            writer.writerow([link])\n",
    "\n",
    "    # access link and read review \n",
    "    try:\n",
    "        df_links = pd.read_csv(INPUT_FILE, header=None) \n",
    "        list_urls = df_links.iloc[:, 0].tolist() \n",
    "        list_urls = [x for x in list_urls if isinstance(x, str) and \"http\" in x]\n",
    "    except Exception as e:\n",
    "        list_urls = []\n",
    "        exit()\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    if list_urls:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_url = {executor.submit(scrape_reviews, url): url for url in list_urls}\n",
    "            \n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_url), total=len(list_urls), desc=\"Crawling Reviews\"):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data:\n",
    "                        all_results.extend(data)\n",
    "                except Exception as exc:\n",
    "                    print(f\"error link : {url} - {exc}\")\n",
    "\n",
    "    # create DataFrame and sort\n",
    "    if all_results:\n",
    "        df_final = pd.DataFrame(all_results)\n",
    "        # sort username\n",
    "        df_final = df_final.sort_values(by=\"user_name\", ascending=True)\n",
    "        # save file\n",
    "        df_final.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        print(df_final.head())\n",
    "    else:\n",
    "        print(\"can not crawl data.\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d1ceba-6aea-4e30-92d7-1aac2180ec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Reviews: 100%|█████████████████████████████████████████████████████████| 5636/5636 [14:13:16<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_name                                         product_id  rating  \\\n",
      "14958            https://www.thegioididong.com/dong-ho-deo-tay/...       4   \n",
      "40096            https://www.thegioididong.com/dong-ho-deo-tay/...       5   \n",
      "70103            https://www.thegioididong.com/dong-ho-deo-tay/...       5   \n",
      "40103            https://www.thegioididong.com/dong-ho-deo-tay/...       5   \n",
      "14948            https://www.thegioididong.com/dong-ho-deo-tay/...       4   \n",
      "\n",
      "                                             comment  \n",
      "14958                              Đeo rất thoải mái  \n",
      "40096                    Nhân viên nhiệt tình vui vẻ  \n",
      "70103  đồng hồ đẹp, vừa tay, hợp với cả nam và nữ ạ.  \n",
      "40103                    Nhân viên nhiệt tình vui vẻ  \n",
      "14948                              Đeo rất thoải mái  \n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        df_links = pd.read_csv(INPUT_FILE, header=None) \n",
    "        list_urls = df_links.iloc[:, 0].tolist() \n",
    "        list_urls = [x for x in list_urls if isinstance(x, str) and \"http\" in x]\n",
    "    except Exception as e:\n",
    "        list_urls = []\n",
    "        exit()\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    if list_urls:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_url = {executor.submit(scrape_reviews, url): url for url in list_urls}\n",
    "            \n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_url), total=len(list_urls), desc=\"Crawling Reviews\"):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data:\n",
    "                        all_results.extend(data)\n",
    "                except Exception as exc:\n",
    "                    print(f\"error link : {url} - {exc}\")\n",
    "\n",
    "    # create DataFrame and sort\n",
    "    if all_results:\n",
    "        df_final = pd.DataFrame(all_results)\n",
    "        # sort username\n",
    "        df_final = df_final.sort_values(by=\"user_name\", ascending=True)\n",
    "        # save file\n",
    "        df_final.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        print(df_final.head())\n",
    "    else:\n",
    "        print(\"can not crawl data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86dab839-0e29-4a7d-8e82-86c3af1ba0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read file\n",
    "df = pd.read_csv('reviews_rating.csv')\n",
    "\n",
    "# fill missing user names \n",
    "missing_mask = df['user_name'].isnull()\n",
    "num_missing = missing_mask.sum()\n",
    "\n",
    "if num_missing > 0:\n",
    "    random_names = [\n",
    "        \"Minh Anh\", \"Thanh Đạt\", \"Hoàng Nam\", \"Ngọc Linh\", \"Đức Huy\",\n",
    "        \"Phương Thảo\", \"Quang Kiệt\", \"Mai Trang\", \"Tuấn Khang\", \"Bảo Châu\"\n",
    "    ]\n",
    "    df.loc[missing_mask, 'user_name'] = np.random.choice(random_names, size=num_missing)\n",
    "else:\n",
    "    print(\"There are no blank rows in the user_name column.\")\n",
    "\n",
    "# extract id and clean name ---\n",
    "# regex captures: Group 1 (digits), Group 2 (name part)\n",
    "# we use 'combine_first' to keep the original name if the regex doesn't match (i.e., no ID prefix)\n",
    "extracted = df['user_name'].str.extract(r'^(\\d+)\\s*[-]*\\s*(.*)$')\n",
    "df['user_id'] = pd.to_numeric(extracted[0], errors='coerce') # Convert captured ID to number\n",
    "df['user_name'] = extracted[1].combine_first(df['user_name']) # Use cleaned name or original\n",
    "\n",
    "# manage id assignment ---\n",
    "# create a map of names that already have an ID {Name: ID}\n",
    "# we drop duplicates so each name appears only once in the map\n",
    "existing_map = df.dropna(subset=['user_id']).drop_duplicates('user_name').set_index('user_name')['user_id']\n",
    "\n",
    "# identify names that are completely missing an ID\n",
    "all_unique_names = df['user_name'].unique()\n",
    "# find names that are NOT in the existing_map index\n",
    "names_needing_id = np.setdiff1d(all_unique_names, existing_map.index)\n",
    "\n",
    "# generate new unique ids \n",
    "if len(names_needing_id) > 0:\n",
    "    # get all IDs currently in use\n",
    "    taken_ids = existing_map.values\n",
    "    \n",
    "    # generate available IDs (10000 to 100000)\n",
    "    # np.setdiff1d removes taken_ids from the full range efficiently\n",
    "    possible_ids = np.arange(10000, 100001)\n",
    "    available_ids = np.setdiff1d(possible_ids, taken_ids)\n",
    "    \n",
    "    if len(available_ids) < len(names_needing_id):\n",
    "        raise ValueError(\"Error: Not enough unique IDs available in the range 10000-100000.\")\n",
    "\n",
    "    # select random unique IDs for the new names\n",
    "    new_ids = np.random.choice(available_ids, size=len(names_needing_id), replace=False)\n",
    "    # create new mapping Series and combine with existing one\n",
    "    new_map = pd.Series(new_ids, index=names_needing_id)\n",
    "    final_map = pd.concat([existing_map, new_map])\n",
    "else:\n",
    "    final_map = existing_map\n",
    "\n",
    "# apply and save\n",
    "# map the final ids back to the dataframe\n",
    "df['user_id'] = df['user_name'].map(final_map).astype(int)\n",
    "\n",
    "# reorder columns (user_id first) and sort\n",
    "df = df[['user_id'] + [c for c in df.columns if c != 'user_id']]\n",
    "df = df.sort_values(by=\"user_id\", ascending=True)\n",
    "\n",
    "# save file\n",
    "df.to_csv('rating.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "339e0911-2dd9-46ab-9b91-818d131c1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96    100971\n",
      "97    100971\n",
      "95    100971\n",
      "98    100971\n",
      "99    100971\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mask = df[\"user_name\"] == \"Lê Quang Phục\"\n",
    "print(df.loc[mask, \"user_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
